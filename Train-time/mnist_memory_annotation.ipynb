{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 Matthieu Courbariaux\n",
    "\n",
    "# This file is part of BinaryNet.\n",
    "\n",
    "# BinaryNet is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "\n",
    "# BinaryNet is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with BinaryNet.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1234)  # for reproducibility\n",
    "\n",
    "# specifying the gpu to use\n",
    "# import theano.sandbox.cuda\n",
    "# theano.sandbox.cuda.use('gpu1') \n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "\n",
    "import binary_net\n",
    "\n",
    "from pylearn2.datasets.mnist import MNIST\n",
    "from pylearn2.utils import serial\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 100\n",
      "alpha = 0.1\n",
      "epsilon = 0.0001\n",
      "num_units = 4096\n",
      "n_hidden_layers = 3\n",
      "num_epochs = 1000\n",
      "dropout_in = 0.2\n",
      "dropout_hidden = 0.5\n",
      "activation = binary_net.binary_tanh_unit\n",
      "binary = True\n",
      "stochastic = False\n",
      "H = 1.0\n",
      "W_LR_scale = Glorot\n",
      "LR_start = 0.003\n",
      "LR_fin = 3e-07\n",
      "LR_decay = 0.990831944893\n",
      "save_path = mnist_parameters.npz\n",
      "shuffle_parts = 1\n"
     ]
    }
   ],
   "source": [
    "# BN parameters\n",
    "batch_size = 100\n",
    "print(\"batch_size = \"+str(batch_size))\n",
    "# alpha is the exponential moving average factor\n",
    "# alpha = .15\n",
    "alpha = .1\n",
    "print(\"alpha = \"+str(alpha))\n",
    "epsilon = 1e-4\n",
    "print(\"epsilon = \"+str(epsilon))\n",
    "\n",
    "# MLP parameters\n",
    "num_units = 4096\n",
    "print(\"num_units = \"+str(num_units))\n",
    "n_hidden_layers = 3\n",
    "print(\"n_hidden_layers = \"+str(n_hidden_layers))\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "print(\"num_epochs = \"+str(num_epochs))\n",
    "\n",
    "# Dropout parameters\n",
    "dropout_in = .2 # 0. means no dropout\n",
    "print(\"dropout_in = \"+str(dropout_in))\n",
    "dropout_hidden = .5\n",
    "print(\"dropout_hidden = \"+str(dropout_hidden))\n",
    "\n",
    "# BinaryOut\n",
    "activation = binary_net.binary_tanh_unit\n",
    "print(\"activation = binary_net.binary_tanh_unit\")\n",
    "# activation = binary_net.binary_sigmoid_unit\n",
    "# print(\"activation = binary_net.binary_sigmoid_unit\")\n",
    "\n",
    "# BinaryConnect\n",
    "binary = True\n",
    "print(\"binary = \"+str(binary))\n",
    "stochastic = False\n",
    "print(\"stochastic = \"+str(stochastic))\n",
    "# (-H,+H) are the two binary values\n",
    "# H = \"Glorot\"\n",
    "H = 1.\n",
    "print(\"H = \"+str(H))\n",
    "# W_LR_scale = 1.    \n",
    "W_LR_scale = \"Glorot\" # \"Glorot\" means we are using the coefficients from Glorot's paper\n",
    "print(\"W_LR_scale = \"+str(W_LR_scale))\n",
    "\n",
    "# Decaying LR \n",
    "LR_start = .003\n",
    "print(\"LR_start = \"+str(LR_start))\n",
    "LR_fin = 0.0000003\n",
    "print(\"LR_fin = \"+str(LR_fin))\n",
    "LR_decay = (LR_fin/LR_start)**(1./num_epochs)\n",
    "print(\"LR_decay = \"+str(LR_decay))\n",
    "# BTW, LR decay might good for the BN moving average...\n",
    "\n",
    "save_path = \"mnist_parameters.npz\"\n",
    "print(\"save_path = \"+str(save_path))\n",
    "\n",
    "shuffle_parts = 1\n",
    "print(\"shuffle_parts = \"+str(shuffle_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inject_batch_size(shape, batch_size):\n",
    "    shape = list(shape)\n",
    "    shape[0] = batch_size\n",
    "    return tuple(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Memory (float)] (100, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input = T.tensor4('inputs')\n",
    "target = T.matrix('targets')\n",
    "LR = T.scalar('LR', dtype=theano.config.floatX)\n",
    "\n",
    "mlp = lasagne.layers.InputLayer(\n",
    "        shape=(None, 1, 28, 28),\n",
    "        input_var=input)\n",
    "print(\"[Data Memory (float)]\", inject_batch_size(mlp.output_shape, batch_size))\n",
    "\n",
    "mlp = lasagne.layers.DropoutLayer(\n",
    "        mlp, \n",
    "        p=dropout_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### hidden layer 0\n",
      "[Parameter Memory (binary)] (784, 4096)\n",
      "[Data Memory (binary)] (100, 4096)\n",
      "### hidden layer 1\n",
      "[Parameter Memory (binary)] (4096, 4096)\n",
      "[Data Memory (binary)] (100, 4096)\n",
      "### hidden layer 2\n",
      "[Parameter Memory (binary)] (4096, 4096)\n",
      "[Data Memory (binary)] (100, 4096)\n"
     ]
    }
   ],
   "source": [
    "prev_dim = 28 * 28\n",
    "\n",
    "for k in range(n_hidden_layers):\n",
    "    print(\"### hidden layer %s\" % k)\n",
    "    mlp = binary_net.DenseLayer(\n",
    "            mlp, \n",
    "            binary=binary,\n",
    "            stochastic=stochastic,\n",
    "            H=H,\n",
    "            W_LR_scale=W_LR_scale,\n",
    "            nonlinearity=lasagne.nonlinearities.identity,\n",
    "            num_units=num_units)\n",
    "    print(\"[Parameter Memory (binary)]\", \"(%s, %s)\" % (prev_dim, num_units))\n",
    "    print(\"[Data Memory (binary)]\", inject_batch_size(mlp.output_shape, batch_size))\n",
    "    prev_dim = num_units # first hidden layer different\n",
    "\n",
    "    mlp = lasagne.layers.BatchNormLayer(\n",
    "            mlp,\n",
    "            epsilon=epsilon, \n",
    "            alpha=alpha)\n",
    "\n",
    "    mlp = lasagne.layers.NonlinearityLayer(\n",
    "            mlp,\n",
    "            nonlinearity=activation)\n",
    "\n",
    "    mlp = lasagne.layers.DropoutLayer(\n",
    "            mlp, \n",
    "            p=dropout_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter Memory (binary)] (4096, 4096)\n",
      "[Data Memory (binary)] (100, 10)\n"
     ]
    }
   ],
   "source": [
    "mlp = binary_net.DenseLayer(\n",
    "            mlp, \n",
    "            binary=binary,\n",
    "            stochastic=stochastic,\n",
    "            H=H,\n",
    "            W_LR_scale=W_LR_scale,\n",
    "            nonlinearity=lasagne.nonlinearities.identity,\n",
    "            num_units=10)\n",
    "print(\"[Parameter Memory (binary)]\", \"(%s, %s)\" % (prev_dim, num_units))\n",
    "print(\"[Data Memory (binary)]\", inject_batch_size(mlp.output_shape, batch_size))\n",
    "\n",
    "mlp = lasagne.layers.BatchNormLayer(\n",
    "        mlp,\n",
    "        epsilon=epsilon, \n",
    "        alpha=alpha)\n",
    "\n",
    "train_output = lasagne.layers.get_output(mlp, deterministic=False)\n",
    "\n",
    "# squared hinge loss\n",
    "loss = T.mean(T.sqr(T.maximum(0.,1.-target*train_output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
